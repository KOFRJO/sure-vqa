from typing import Optional

from huggingface_hub import login
from transformers import pipeline
import os
import json
import torch
from tqdm import tqdm
from pathlib import Path
import re

from med_vlm_robustness.utils import set_seed


def get_score_output_dict(model_output, score):
    return {
        "qid": model_output["qid"],
        "question": model_output["question"],
        "gt": model_output["gt"],
        "pred": model_output["pred"],
        "answer_type": model_output["answer_type"],
        "score": score
    }


def get_score_from_llm_output(qid, llm_output):
    try:
        # get the score value in the model output
        if "\"" not in llm_output:
            llm_output = llm_output.replace("score", "\"score\"")
        score_obj = json.loads("{" + re.search("\"score\":(\s*)(\d*\.?\d*)", llm_output).group(0) + "}")
        score = score_obj["score"]
        # score = score_obj_float
    except ValueError:
        # Handle the case where the pattern is not found in the outputs
        # Where the score is not numeric therefore it cannot be converted to float()
        # Make the score store the complete output so that it can be analyzed later
        print("Retrieved score object is being converted to float but it is not numeric")
        print(
            f"Score for the instance {qid} will be assigned to the complete text output of the model.")
        print("Inspect this in the output JSON file")
        score = llm_output
    except Exception as e:
        # Handle other potential errors
        print(
            f"Score for the instance {qid} will be assigned to the complete text output of the model.")
        print("Inspect this in the output JSON file")
        print("This is the original error message:\n", e)
        score = llm_output
    return score


def do_inference(line, pipe, initial_prompt):
    qid = line["qid"]
    complete_input = "<|endoftext|><|user|>\n" + initial_prompt + str(line) + " <|end|>\n<|assistant|>"
    messages = [
        {
            "role": "user",
            "content": complete_input,
        },
    ]
    outputs = pipe(messages, max_new_tokens=256)
    output = outputs[0]["generated_text"][-1]["content"].strip()
    score = get_score_from_llm_output(qid, output)
    output_dict = get_score_output_dict(line, score)
    return output_dict


def gemma_eval(model_output_file, eval_file:Optional[str] = None, closed=False, multilabel=False, data_categories=None,
               seed: int = 123):
    set_seed(seed)
    model_id = "google/gemma-2-9b-it"
    # Load the output JSON data generated by the model
    with open(model_output_file, 'r') as file:
        model_output_data = json.load(file)

    # read the initial prompt
    if not closed:
        prompt_file_path = os.getenv("PROMPT_FILE_PATH")
    elif closed and not multilabel:
        prompt_file_path = os.getenv("PROMPT_FILE_PATH_CLOSED")
    else:
        prompt_file_path = os.getenv("PROMPT_FILE_PATH_MULTILABEL")
    with open(prompt_file_path, 'r') as f:
        initial_prompt = f.read()
    login(token=os.getenv("HUGGINGFACE_TOKEN"))
    pipe = pipeline(
        "text-generation",
        model=model_id,
        model_kwargs={"torch_dtype": torch.bfloat16},
        device="cuda",  # replace with "mps" to run on a Mac device
    )

    score_output_list = []

    for line in tqdm(model_output_data):
        if not closed:
            if line["answer_type"] == "CLOSED":
                continue
            if line["answer_type"] == "OPEN":
                if line["pred"] == line["gt"]:
                    score_output_list.append(get_score_output_dict(line, 5))
                else:
                    score_output_list.append(do_inference(line, pipe, initial_prompt))
        else:
            if line["answer_type"] == "OPEN":
                continue
            if line["answer_type"] == "CLOSED":
                if data_categories is not None:
                    categories = data_categories[data_categories['qid'] == line["qid"]]['list_categories'].iloc[0]
                else:
                    categories = None
                if categories is None:
                    if line["pred"] == line["gt"]:
                        score_output_list.append(get_score_output_dict(line, 1))
                    else:
                        score_output_list.append(do_inference(line, pipe, initial_prompt))
                elif len(categories) == 2 and not multilabel:
                    if line["pred"] == line["gt"]:
                        score_output_list.append(get_score_output_dict(line, 1))
                    else:
                        score_output_list.append(do_inference(line, pipe, initial_prompt))
                elif len(categories) > 2 and not multilabel:
                    continue
                elif len(categories) > 2 and multilabel:
                    if line["pred"] == line["gt"]:
                        score_output_list.append(get_score_output_dict(line, 1))
                    else:
                        score_output_list.append(do_inference(line, pipe, initial_prompt))
    score_output_list = average_metrics(score_output_list, closed)
    if eval_file is not None:
        # save evaluation as JSON
        if not Path(eval_file).parent.is_dir():
            os.makedirs(Path(eval_file).parent)
        with open(eval_file, 'w') as json_file:
            json.dump(score_output_list, json_file, indent=4)
    else:
        return score_output_list


def average_metrics(score_output_list, closed):
    sum_open_ended_score = 0
    num_open_qs=0
    sum_closed_ended_score = 0
    num_closed_qs=0
    for object in score_output_list:
        question_id = object["qid"]
        answer_type=object["answer_type"]
        score=object["score"]

        try:
            float_score = float(score)

            if not closed:
                if answer_type == "OPEN":
                    sum_open_ended_score += float_score
                    num_open_qs += 1
                else:
                    continue
            else:
                if answer_type == "CLOSED":
                    sum_closed_ended_score += float_score
                    num_closed_qs += 1
                else:
                    continue
        except ValueError:
            print(f"score is not numeric for the question id {question_id}, "
                  f"this instance will be skipped during average score calculation")
            print("If this is not desired, check the metrics JSON file to fix the error")

    if not closed:
        average_scores = {
            "avg_score": sum_open_ended_score / max(1, num_open_qs)
        }
    else:
        average_scores = {
            "avg_score": sum_closed_ended_score / max(1, num_closed_qs)
        }
    score_output_list.insert(0, average_scores)

    return score_output_list
